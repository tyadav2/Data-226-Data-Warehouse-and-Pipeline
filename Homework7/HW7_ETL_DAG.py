# -*- coding: utf-8 -*-
"""Data226_HW7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P4VJkT4lJ0kn3DqC7_eBusz6NIVPKbNi
"""

from airflow import DAG
from airflow.models import Variable
from airflow.decorators import task
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook

from datetime import timedelta
from datetime import datetime
import snowflake.connector
import requests

def return_snowflake_conn():
    # Initialize the SnowflakeHook
    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')

    # Execute the query and fetch results
    conn = hook.get_conn()
    return conn.cursor()

@task
def create_table():
    cur = return_snowflake_conn()
    try:
        cur.execute("BEGIN;")
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS data226.raw_data.user_session_channel (
            userId int not NULL,
            sessionId varchar(32) primary key,
            channel varchar(32) default 'direct'
            );
        """)

        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS data226.raw_data.session_timestamp (
            sessionId varchar(32) primary key,
            ts timestamp
            );
        """)

        cur.execute("COMMIT;")

    except Exception as e:
        cur.execute("ROLLBACK;")
        print(e)
        raise e

    finally:
        cur.close()

@task
def populate_table():
    # for the following query to run,
    # the S3 bucket should have LIST/READ privileges for everyone

    cur = return_snowflake_conn()
    try:
        cur.execute("BEGIN;")
        cur.execute(f"""
            CREATE OR REPLACE STAGE data226.raw_data.blob_stage
            url = 's3://s3-geospatial/readonly/'
            file_format = (type = csv, skip_header = 1, field_optionally_enclosed_by = '"');
        """)

        sql = f"""
                COPY INTO data226.raw_data.user_session_channel
                FROM @data226.raw_data.blob_stage/user_session_channel.csv;
            """
        sql2 = f"""
                COPY INTO data226.raw_data.session_timestamp
                FROM @data226.raw_data.blob_stage/session_timestamp.csv;
            """
        cur.execute(sql)
        cur.execute(sql2)
        cur.execute("COMMIT;")

    except Exception as e:
        cur.execute("ROLLBACK;")
        print(e)
        raise e

    finally:
        cur.close()

with DAG(
    dag_id='Homework8',
    start_date=datetime(2024, 10, 23),
    catchup=False,
    schedule='@daily',  # Adjust the schedule as needed
    tags=['ETL', 'data226', 'hw', 'snowflake']
) as dag:

    create_task = create_table()
    populate_task = populate_table()

    create_task >> populate_task
